<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#fff">


<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<style>
    .pace .pace-progress {
        background: #ff009e; /*进度条颜色*/
        height: px;
    }
    .pace .pace-progress-inner {
         box-shadow: 0 0 10px #0000FF, 0 0 5px #0000FF; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #0000FF;    /*上边框颜色*/
        border-left-color: #0000FF;    /*左边框颜色*/
    }
</style>





  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-loading-bar.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



















  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  

  
    
      
    

    
  

  
    
    
    <link href="https://fonts.loli.net/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Roboto Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.4.2" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=6.4.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=6.4.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=6.4.2">


  <link rel="mask-icon" href="/images/safari_pinned_tab.svg?v=6.4.2" color="#fff">



  <meta name="msapplication-config" content="/images/browserconfig.xml" />







<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    version: '6.4.2',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":true,"onmobile":true},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="机器学习与数据挖掘复习">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习与数据挖掘">
<meta property="og:url" content="https://wheliosc.github.io/mldm.html">
<meta property="og:site_name" content="李三岁他很皮">
<meta property="og:description" content="机器学习与数据挖掘复习">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-12-19T13:03:16.535Z">
<meta property="article:modified_time" content="2021-08-30T08:41:15.804Z">
<meta property="article:author" content="lihui">
<meta property="article:tag" content="数据挖掘">
<meta name="twitter:card" content="summary">



  <link rel="alternate" href="/atom.xml" title="李三岁他很皮" type="application/atom+xml" />




  <link rel="canonical" href="https://wheliosc.github.io/mldm.html"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>机器学习与数据挖掘 | 李三岁他很皮</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">李三岁他很皮</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">学习干嘛 愣着啊</p>
      
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />标签</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-music">
    <a href="/music/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-music"></i> <br />music</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-paw"></i> <br />关于</a>
  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wheliosc.github.io/mldm.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="lihui">
      <meta itemprop="description" content="星光即使微弱也会为我照亮前途">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="李三岁他很皮">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习与数据挖掘
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-12-19 21:03:16" itemprop="dateCreated datePublished" datetime="2020-12-19T21:03:16+08:00">2020-12-19</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2021-08-30 16:41:15" itemprop="dateModified" datetime="2021-08-30T16:41:15+08:00">2021-08-30</time>
              
            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/mldm.html#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/mldm.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon"
            >
            <i class="fa fa-eye"></i>
             阅读次数： 
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">本文字数：</span>
                
                <span title="本文字数">33k</span>
              

              

              
            </div>
          

          
              <div class="post-description">机器学习与数据挖掘复习</div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><p></p><br><span id="more"></span></p>
<h1 id="第一章-引言"><a href="#第一章-引言" class="headerlink" title="第一章 引言"></a>第一章 引言</h1><h2 id="0-什么是数据挖掘"><a href="#0-什么是数据挖掘" class="headerlink" title="0. 什么是数据挖掘"></a>0. 什么是数据挖掘</h2><p>数据挖掘是从大量数据中挖掘出有趣模式和知识的过程或方法，其中涉及机器学习、统计数据和数据库系统交叉处的方法。</p>
<h2 id="1-数据中的知识发现包括哪几个步骤"><a href="#1-数据中的知识发现包括哪几个步骤" class="headerlink" title="1. 数据中的知识发现包括哪几个步骤"></a>1. 数据中的知识发现包括哪几个步骤</h2><ul>
<li>business understanding（业务理解）</li>
<li>data understanding（数据理解）</li>
<li>data preparation（数据准备）</li>
<li>modeling（建模）</li>
<li>evaluation（评估）</li>
<li>development（部署）</li>
</ul>
<h2 id="2-数据挖掘的应用"><a href="#2-数据挖掘的应用" class="headerlink" title="2. 数据挖掘的应用"></a>2. 数据挖掘的应用</h2><p>商务智能、Web搜索、生物信息学、卫生保健信息学、金融、数字图书馆和数字政府等。</p>
<h1 id="第二章-学习的可行性"><a href="#第二章-学习的可行性" class="headerlink" title="第二章 学习的可行性"></a>第二章 学习的可行性</h1><h2 id="1-Hoeffding’s-Inequality（霍夫汀不等式）"><a href="#1-Hoeffding’s-Inequality（霍夫汀不等式）" class="headerlink" title="1. Hoeffding’s Inequality（霍夫汀不等式）"></a>1. Hoeffding’s Inequality（霍夫汀不等式）</h2><p>$P(| v- \mu | &gt; \epsilon) \leq 2 e^{-2 \epsilon^2 N}$</p>
<p>霍夫丁不等式说明，抽样比例$v$与总体比例$\mu$的差距大于某个边距值$\epsilon$的概率，小于等于一个由$\epsilon$和样本大小$N$得到的关系式的值。</p>
<h2 id="2-用霍夫汀不等式说明学习的可行性"><a href="#2-用霍夫汀不等式说明学习的可行性" class="headerlink" title="2. 用霍夫汀不等式说明学习的可行性"></a>2. 用霍夫汀不等式说明学习的可行性</h2><p>在统计学中我们通过霍夫丁不等式来限定抽样集与总体集中的频率与概率关系。通过将其推广到机器学习中，可以得到类似的单个hypothesis的$E_{in}$与$E_{out}$差距限定不等式。然而当考虑到一组hypothesis数量多的假设集时，即使对单个hypothesis遇到bad data使其$E_{in}$与$E_{out}$差别比较大的概率十分低，但总体而言其中有一个或几个hypothesis遇到bad data使其$E_{in}$与$E_{out}$差别较大。我们将假设集中假设的数量$M$通过union bound引入不等式中，最终得到$P_D(BAD D) \leq 2M e^{-2 \epsilon^2 N}$。在<strong>有限的</strong>假设数量时，只要样本数量$N$足够，我们依然能让这组data保证取到的hypothesis的$E_{in}$与$E_{out}$差距不大，从而可以挑选$E_{in}$最小的hypothesis，也即$E_{out}$最小的hypothesis，作为结果。</p>
<h1 id="第三章-数据和数据预处理"><a href="#第三章-数据和数据预处理" class="headerlink" title="第三章 数据和数据预处理"></a>第三章 数据和数据预处理</h1><h2 id="1-属性类型和可进行的操作"><a href="#1-属性类型和可进行的操作" class="headerlink" title="1. 属性类型和可进行的操作"></a>1. 属性类型和可进行的操作</h2><div class="table-container">
<table>
<thead>
<tr>
<th>属性类型</th>
<th>描述</th>
<th>操作</th>
<th>举例</th>
</tr>
</thead>
<tbody>
<tr>
<td>标称属性（nominal）</td>
<td>仅仅是不同的名字，用于区分属性（$=, \neq$）</td>
<td>众数，熵，列联相关，$\chi^2$检验</td>
<td>邮政编码，员工ID，性别</td>
</tr>
<tr>
<td>序数属性（ordinal）</td>
<td>序数属性的值可以确定对象的序（$&lt;, &gt;$）</td>
<td>中值，百分位</td>
<td>矿石硬度（好，较好，最好）</td>
</tr>
<tr>
<td>区间属性（interval）</td>
<td>值之间的差是有意义的（$+, -$）</td>
<td>均值，标准差，皮尔逊相关</td>
<td>日历日期，摄氏温度</td>
</tr>
<tr>
<td>比率属性（ratio）</td>
<td>差和比率都有意义（$\times, \div$）</td>
<td>几何平均，调和平均，百分比变差</td>
<td>质量，长度，绝对温度</td>
</tr>
</tbody>
</table>
</div>
<p>标称属性和序数属性统称为分类的或定性的属性，区间属性和比率属性统称为定量的或数值属性。</p>
<h2 id="2-非对称属性"><a href="#2-非对称属性" class="headerlink" title="2. 非对称属性"></a>2. 非对称属性</h2><p>属性只有两个类别或状态，用0和1编码。如果他的两种状态具有同等的价值并且有相同的权重，则称为对称的二元属性，比如男女性别。如果其状态的结果不是同样重要的，其中更关注1编码的结果（通常是稀疏的），称其为非对称的二元属性，比如covid-19核酸检测的阴阳性结果。</p>
<h2 id="3-相似性和相异性度量"><a href="#3-相似性和相异性度量" class="headerlink" title="3. 相似性和相异性度量"></a>3. 相似性和相异性度量</h2><h3 id="3-1-数据对象之间的相异度"><a href="#3-1-数据对象之间的相异度" class="headerlink" title="3.1 数据对象之间的相异度"></a>3.1 数据对象之间的相异度</h3><ul>
<li>欧几里得距离$d=\sqrt {\Sigma_{k=1}^{n}(p_k-q_k)^2}$。</li>
</ul>
<ul>
<li>闵可夫斯基距离$d=\lgroup \Sigma_{k=1}^{n}|(p_k-q_k)|^r\rgroup^{\frac{1}{r}}$是欧氏距离的推广。当$r=1$时，称为曼哈顿距离（$L_1$范数）；当$r=2$时，称为欧几里得距离（$L_2$范数）；当$r=\infty$时，称为切比雪夫距离（上确界距离，$L_{max}$范数，$L_\infty$范数），它是对象属性间的最大距离。</li>
</ul>
<ul>
<li>马氏距离$d_m(x) = \sqrt{(x-\mu) \Sigma^{-1} (x-\mu)^T}$，其中$x$是一个均值为$\mu$的协方差矩阵为$\Sigma$的多变量矢量。对两个服从统一分布并且协方差矩阵为$\Sigma$的变量（样本）$p$和$q$来说，其差异程度可以由马氏距离表示为$d_m(p,q)= \sqrt{(p-q) \Sigma^{-1} (p-q)^T}$。马氏距离排除了变量之间的相关性的干扰，且尺度无关（不受量纲影响）。</li>
</ul>
<h3 id="3-2-二元数据的相似性度量"><a href="#3-2-二元数据的相似性度量" class="headerlink" title="3.2 二元数据的相似性度量"></a>3.2 二元数据的相似性度量</h3><ul>
<li>简单匹配系数（SMC）：$SMC = \frac{f_{11}+f_{00}}{f_{00}+f_{10}+f_{01}+f_{11}}$</li>
</ul>
<ul>
<li>Jaccard系数：$J = \frac{f_{11}}{f_{10}+f_{01}+f_{11}}$</li>
</ul>
<ul>
<li>广义Jaccard系数（Tinamoto系数）：$EJ(p,q) = \frac{p \cdot q}{\parallel p \parallel ^2 + \parallel q \parallel ^2 - p \cdot q}$</li>
</ul>
<ul>
<li>余弦相似度：$cos(x,y) = \frac{x \cdot y}{\parallel p \parallel \times \parallel q \parallel}$</li>
</ul>
<ul>
<li>相关系数：$r(X, Y)=\frac{\operatorname{Cov}(X, Y)}{\sqrt{\operatorname{Var}[X] \operatorname{Var}[Y]}}$或$r(X, Y)=\frac{\operatorname{Cov}(X, Y)}{\sigma_X \sigma_Y}$。</li>
</ul>
<h3 id="3-3-组合异种属性的相似度"><a href="#3-3-组合异种属性的相似度" class="headerlink" title="3.3 组合异种属性的相似度"></a>3.3 组合异种属性的相似度</h3><ul>
<li>在第$k$个属性上，计算相似度$s_k(p,q)$，在区间$[0,1]$中。</li>
</ul>
<ul>
<li>对于第$k$个属性，定义一个指示变量$\delta_k$，如果第$k$个属性是非对称属性，并且两个对象在该属性的值都是0，或者如果有一个对象的第$k$个属性具有缺失值，则$\delta_k = 0$，否则$\delta_k = 1$。</li>
</ul>
<ul>
<li>总相似度为$similarity(p,q) = \frac{\Sigma_{k=1}^{n}\delta_k s_k (p,q)}{\Sigma_{k=1}^{n}\delta_k}$。</li>
</ul>
<ul>
<li>当某些属性更重要时，可以使用不同权值对不同属性的相似度进行加权，则上式变为$similarity(p,q) = \frac{\Sigma_{k=1}^{n} w_k \delta_k s_k (p,q)}{\Sigma_{k=1}^{n}\delta_k}$。</li>
</ul>
<h2 id="4-数据预处理"><a href="#4-数据预处理" class="headerlink" title="4. 数据预处理"></a>4. 数据预处理</h2><p>主要任务：数据清理（data cleaning）、数据集成（data integration）、数据规约（data reduction）、数据变换（data transformation）、数据离散化（data discretization）。</p>
<h3 id="4-1-数据清理"><a href="#4-1-数据清理" class="headerlink" title="4.1 数据清理"></a>4.1 数据清理</h3><h4 id="4-1-1-填充缺失值（missing-data）"><a href="#4-1-1-填充缺失值（missing-data）" class="headerlink" title="4.1.1 填充缺失值（missing data）"></a>4.1.1 填充缺失值（missing data）</h4><ul>
<li>忽略元组</li>
<li>人工填写缺失值</li>
<li>使用一个全局常量填充缺失值</li>
<li>使用属性的中心度量（如均值和中位数）填充缺失值</li>
<li>使用与给定元组属同一类的所有样本的属性均值或中位数填充缺失值</li>
<li>使用最可能的值填充缺失值</li>
</ul>
<h4 id="4-1-2-光滑噪声（noise）"><a href="#4-1-2-光滑噪声（noise）" class="headerlink" title="4.1.2 光滑噪声（noise）"></a>4.1.2 光滑噪声（noise）</h4><ul>
<li><p>分箱（binning）</p>
<ul>
<li>等宽分箱：将变量的取值范围分为$k$个等宽的区间，每个区间当作一个分箱。</li>
<li>等频（深）分箱：分为$k$个区间，每个区间中的样本数大体相同。</li>
<li>分箱之后，可以使用箱均值平滑，箱中位数平滑，箱边界平滑（箱中的每一个值取离它最近的边界值，即最大值或最小值，与最小值的差小则取最小值，与最大值的差小则取最大值）。</li>
</ul>
</li>
<li><p>回归（regression）</p>
<ul>
<li>用一个函数拟合数据来光滑数据</li>
</ul>
</li>
</ul>
<h4 id="4-1-3-识别离群点（outlier）"><a href="#4-1-3-识别离群点（outlier）" class="headerlink" title="4.1.3 识别离群点（outlier）"></a>4.1.3 识别离群点（outlier）</h4><ul>
<li><p>聚类（cluster）</p>
<ul>
<li>落在簇之外的值为离群点</li>
</ul>
</li>
<li><p>曲线拟合</p>
</li>
<li>给定模型上的假设检验</li>
</ul>
<h4 id="4-1-4-纠正数据中的不一致"><a href="#4-1-4-纠正数据中的不一致" class="headerlink" title="4.1.4 纠正数据中的不一致"></a>4.1.4 纠正数据中的不一致</h4><h3 id="4-2-数据集成"><a href="#4-2-数据集成" class="headerlink" title="4.2 数据集成"></a>4.2 数据集成</h3><p>数据集成将多个数据源的数据合并，存放在一个一致的数据存储中，如数据仓库中。</p>
<h4 id="4-2-1-实体识别问题"><a href="#4-2-1-实体识别问题" class="headerlink" title="4.2.1 实体识别问题"></a>4.2.1 实体识别问题</h4><p>数据分析者或计算机如何才能确信一个数据库中的某个属性与另一个数据库中的某个属性指的是相同的属性。</p>
<h4 id="4-2-2-冗余和相关分析"><a href="#4-2-2-冗余和相关分析" class="headerlink" title="4.2.2 冗余和相关分析"></a>4.2.2 冗余和相关分析</h4><p>一个属性如果可以由另一个或另一组属性导出，则这个属性可能就是冗余的。有些冗余可以被相关分析检测到。对于分类的属性，使用$\chi ^2$（卡方）检验。对于数值属性（numerical），使用相关系数和协方差。</p>
<ul>
<li><p>$\chi ^2检验$</p>
<ul>
<li>假设对两个属性$A$和$B$，$A$有$c$个不同的值，$B$有$r$个不同的值，可以得到一个$c \times r$的列联表。令$(A_i,B_j)$表示属性$A$取值$a_i$、属性$B$取值$b_j$的联合事件，$\chi ^2$值可以通过公式计算</li>
<li>$\chi ^2 = \Sigma_{i=1}^{c} \Sigma_{j=1}^{r} \frac{(o_{ij} - e_{ij})^2}{e_{ij}}$。其中，$o_{ij}$是联合事件$(A_i,B_j)$的观测频度（即实际计数），$e_{ij}$是$(A_i,B_j)$的期望频度。</li>
<li>$e_{ij}$的计算公式为$e_{ij} = \frac{count(A=a_i) \times count(B=b_j)}{n}$。其中，$n$是元组的个数，$count(A=a_i)$是$A$上具有值$a_i$的元组个数，$count(B=b_j)$是$B$上具有值$b_i$的元组个数。</li>
<li>对卡方值贡献最大的单元是实际计数和期望计数很不相同的单元。</li>
<li>卡方检验假设$A$和$B$是独立的，检测基于显著水平，具有自由度$(c-1) \times (r-1)$。</li>
</ul>
</li>
<li><p>协方差</p>
<ul>
<li>$cov(A,B) = E((A-\overline{A})(B-\overline{B})) = \frac{\Sigma_{i=1}^{n} (a_i - \overline{A}) ((b_i - \overline{B})}{n} = E(A \cdot B) - \overline{A} \overline{B}$</li>
</ul>
</li>
<li><p>相关系数（皮尔森相关系数）</p>
<ul>
<li>公式为$r_{A,B} = \frac{\Sigma_{i=1}^{n} (a_i - \overline{A}) ((b_i - \overline{B})}{n \sigma_A \sigma_B}=\frac{\Sigma_{i=1}^{n} (a_i b_i) - n \overline{A} \overline{B}}{n \sigma_A \sigma_B}$</li>
<li>通过协方差的定义，相关系数公式还可以写为$r_{A,B}=\frac{\operatorname{Cov}(A, B)}{\sigma_A \sigma_B}$</li>
<li>$r_{A,B} \in [-1,+1]$。如果$r_{A,B}&gt;0$，则$A$和$B$是正相关的，该值越大，相关性越强。如果$r_{A,B}=0$，则$A$和$B$是不相关的。</li>
<li>独立可推出不相关，但不相关并不能推出独立。不相关是指两个随机变量没有近似的线性关系，而独立是指两个变量没有任何关系。</li>
</ul>
</li>
</ul>
<h3 id="4-3-数据归约"><a href="#4-3-数据归约" class="headerlink" title="4.3 数据归约"></a>4.3 数据归约</h3><p>数据归约可以得到数据集的简约表示，它小得多，在规约后的数据集上挖掘将产生相同或几近相同的分析结果。</p>
<p>可以使用的策略：数据聚合（data aggregation）、数据压缩（data compression）、数量规约（numerosity reduction）、维归约（dimensionality reduction）、离散化和概念层次生成（Discretization and concept hierarchy generation）等。</p>
<h4 id="4-3-1-数据聚合"><a href="#4-3-1-数据聚合" class="headerlink" title="4.3.1 数据聚合"></a>4.3.1 数据聚合</h4><p>把两个或者多个属性组合成单个属性。</p>
<h4 id="4-3-2-数据压缩"><a href="#4-3-2-数据压缩" class="headerlink" title="4.3.2 数据压缩"></a>4.3.2 数据压缩</h4><p>使用变换得到原数据的规约表示或压缩表示。如果原数据可以从压缩后的数据中重构而不损失信息则称该数据归约是无损的，如果只能近似的重构原数据，则该数据归约称为有损的。</p>
<h4 id="4-3-3-数量规约"><a href="#4-3-3-数量规约" class="headerlink" title="4.3.3 数量规约"></a>4.3.3 数量规约</h4><p>用替代的、较小的数据表示替换原数据，分为参数化方法和非参数化方法。参数方法使用模型估计数据，因此只需要存放模型参数（离群点可能也需要存放），回归模型就是一个例子。非参数化方法包括直方图（histograms）、聚类（clustering）、抽样（sampling）等。</p>
<h4 id="4-3-4-维归约"><a href="#4-3-4-维归约" class="headerlink" title="4.3.4 维归约"></a>4.3.4 维归约</h4><h3 id="4-4-数据变换与数据离散化"><a href="#4-4-数据变换与数据离散化" class="headerlink" title="4.4 数据变换与数据离散化"></a>4.4 数据变换与数据离散化</h3><p>数据变换策略包括：光滑（smoothing）、属性构造、聚集、规范化、离散化等。数据预处理任务之间存在着重叠，因此这一部分只讨论规范化和离散化。</p>
<h4 id="4-4-1-规范化"><a href="#4-4-1-规范化" class="headerlink" title="4.4.1 规范化"></a>4.4.1 规范化</h4><p>规范化可以避免对度量单位选择的依赖性。对于基于距离的方法，规范化可以帮助防止具有较大初始值域的属性和具有较小初始值域的属性相比权重过大。</p>
<ul>
<li><p>最小-最大规范化</p>
<ul>
<li>假设$min_A$和$max_A$是属性$A$的最小值和最大值，要将属性$A$规范化到区间$[ new _ min_A, new _ max_A]$中，$a_i$的规范后结果为</li>
<li>$a_i \prime = \frac{a_i - min_A}{max_A - min_A} \times (new _ max_A - new _ min_A) + new _ min_A$</li>
</ul>
</li>
<li><p>$z-score$规范化</p>
<ul>
<li>$a_i \prime = \frac{a_i - \overline{A}}{\sigma_A}$</li>
</ul>
</li>
<li><p>小数定标规范化</p>
<ul>
<li>$a_i \prime = \frac{a_i}{10^j}$，其中$j$是使得$max(|a_i \prime |)&lt;1$的最小整数。</li>
</ul>
</li>
</ul>
<h4 id="4-4-2-离散化"><a href="#4-4-2-离散化" class="headerlink" title="4.4.2 离散化"></a>4.4.2 离散化</h4><p>对数值数据进行离散化，根据不同的标准可以划分为监督的和非监督的、自顶向下的和自底向上的（分裂的和合并的）。</p>
<p>常用的方法有：</p>
<ul>
<li>直方图分析（自顶向下的，分裂的，非监督的）</li>
<li>聚类分析（自顶向下的或自底向上的、分裂的或合并的，非监督的）</li>
<li>基于熵的离散化（自顶向下的，分裂的，监督的）</li>
<li>通过$\chi ^2$分析合并区间（<strong>自底向上的</strong>，合并的，监督的）</li>
<li>自然分割（自顶向下的，分裂的，非监督的）<ul>
<li>3-4-5规则可以用于将数值数据划分成相对一致、“自然的”区间。一般地，该规则根据最重要的数字上（比如数字的最高位）的值区域，递归地、逐层地将给定的数据区域划分为3、4 或5 个等长的区间。</li>
<li>以最高位数字举例来说，如果在所有数字的最高位覆盖3, 6, 7或9个不同的值，则将数据分成3段；如果在所有数字的最高位覆盖2, 4, 8个不同的值，则将数据分成4段；如果在所有数字的最高位覆盖1, 5, 10个不同的值，则将数据分成5段。</li>
</ul>
</li>
</ul>
<h1 id="第四章-决策树学习"><a href="#第四章-决策树学习" class="headerlink" title="第四章 决策树学习"></a>第四章 决策树学习</h1><h2 id="1-决策树学习的基本思想"><a href="#1-决策树学习的基本思想" class="headerlink" title="1. 决策树学习的基本思想"></a>1. 决策树学习的基本思想</h2><p>决策树是一个树结构，其每一个非叶节点表示一个属性上的测试，每个分支代表该测试的一个输出，每个叶节点存放一个类别。决策树学习的基本思想是对所有的属性进行评估，选择一个最好的属性作为树的根节点，然后为该属性的每个可能值创建划分节点，并将数据集按取值划分到不同的节点上，然后用与每个子结点相关联的训练实例重复整个过程，以选择树中该点处要测试的最佳属性。</p>
<h2 id="2-如何选择最佳划分"><a href="#2-如何选择最佳划分" class="headerlink" title="2. 如何选择最佳划分"></a>2. 如何选择最佳划分</h2><p>为了决定一个最佳划分，需要对节点进行不纯性度量，理想的情况是每个分区应当是纯的（落在一个给定分区的所有元组都属于相同的类）。</p>
<p>不纯性度量包括基尼（Gini Index）指数、熵（Entropy）和分类错误率（Misclassification error）等。</p>
<ul>
<li><p>基尼指数</p>
<ul>
<li>一个节点的基尼指数定义为$Gini(node) = 1 - \Sigma_{i=1}^{n} p_i^2$，其中$p_i$是$node$中元组属于$Class_i$类的概率。</li>
<li>如果将一个节点$D$分裂成了$k$个部分（子节点），对这个划分来说，$D$的基尼指数为$Gini_{split}(D) = \Sigma_{i=1}^{k} \frac{n_i}{n}Gini(i)$，其中$n_i$是属于某一个子节点的元组的个数，$n$是属于节点$D$的元组数。</li>
<li>对于离散属性来说，选择分裂后的基尼指数小于未分裂的基尼指数且基尼指数最小的划分。</li>
<li>对于连续属性来说，对属性的可能取值进行排序，然后将每对相邻值的中点作为可能的分裂点，如果是二元划分，则选择产生最小基尼指数的点作为该属性的分裂点。对于分裂点$split _ point$来说，它产生的两个数据子集是$\leq split _ point$和$&gt; split _ point$。</li>
<li>基尼指数应用于CART（Classification and Regression Trees）算法中。</li>
</ul>
</li>
<li><p>熵</p>
<ul>
<li>$Entropy(node) = - \Sigma_{i=1}^{n} p_i log_2(p_i)$，其中$p_i$是$node$中元组属于$Class_i$类的概率。</li>
<li>熵越大表示区分类别需要的信息越多，则节点内的纯度越低。</li>
<li>信息增益：$Gain_{split}(A) = Entropy(node) - (\Sigma_{i=1}^k \frac{n_i}{n} Entropy(i))$，表示将一个节点按属性$A$分为$k$个部分后得到的信息增益，选择具有最大信息增益的属性进行划分。信息增益应用于ID3算法中。（倾向于产生大量的分区，使每一个值有一个分区，这样每一个分区都是纯的，但这种划分没用。）</li>
<li>信息增益率：$GainRate(A) = \frac{Gain(A)}{SplitInfo_A(D)}$，其中$SplitInfo_A(D) = -\Sigma_{i=1}^k \frac{n_i}{n} log_2(\frac{n_i}{n})$。信息增益率应用于C4.5算法中。</li>
</ul>
</li>
<li><p>分类错误率</p>
<ul>
<li>$Error = 1 - max(p_i)$，其中$p_i$是$node$中元组属于$Class_i$类的概率。</li>
</ul>
</li>
</ul>
<h2 id="3-过拟合和欠拟合"><a href="#3-过拟合和欠拟合" class="headerlink" title="3. 过拟合和欠拟合"></a>3. 过拟合和欠拟合</h2><ul>
<li><p>过拟合</p>
<ul>
<li>过拟合是指模型把数据学习的太彻底，以至于把噪声或没有代表性的数据的特征也学习到了，这样就会导致在后期测试的时候不能够很好的识别数据，即不能正确的分类，模型的泛化能力差。</li>
<li>引起过拟合的原因包括数据集中有噪声数据或训练样例太少以至于不能产生目标函数的有代表性的采样。</li>
<li>解决决策树学习中过拟合的方法：预剪枝（及早停止树增长）和后剪枝（允许树过度拟合数据，之后对树进行修剪）。</li>
</ul>
</li>
<li><p>欠拟合</p>
<ul>
<li>欠拟合是指模型的拟合程度不高，模型没有很好地捕捉到数据特征，不能够很好地拟合数据。</li>
</ul>
</li>
</ul>
<h2 id="4-缺失值对决策树的影响"><a href="#4-缺失值对决策树的影响" class="headerlink" title="4. 缺失值对决策树的影响"></a>4. 缺失值对决策树的影响</h2><p>决策树对缺失值不敏感。</p>
<ul>
<li>删除有缺失值的数据，或者不考虑有缺失值的属性来生成决策树。</li>
<li>默认将含有缺失值的实例划分到某一子树。</li>
</ul>
<h2 id="5-混淆矩阵（confusion-matrix）"><a href="#5-混淆矩阵（confusion-matrix）" class="headerlink" title="5. 混淆矩阵（confusion matrix）"></a>5. 混淆矩阵（confusion matrix）</h2><p>简记Actual Class为AC，Predicted Class为PC</p>
<ul>
<li>TP: (true positive)  AC=yes, PC=yes</li>
<li>FN: (false negative) AC=yes, PC=no</li>
<li>FP: (fasle positive) AC=no, PC=yes</li>
<li>TN: (true negative)  AC=no, PC=no</li>
</ul>
<p>准确率：$Accuracy = \frac{TP+TN}{TP+FN+FP+TN}$</p>
<p>精确率：$Precision = \frac{TP}{TP+FP}$</p>
<p>召回率：$Recall = \frac{TP}{TP+FN}$</p>
<p>$F_1$分数：$F_1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$</p>
<h2 id="6-评估分类器性能的方法"><a href="#6-评估分类器性能的方法" class="headerlink" title="6. 评估分类器性能的方法"></a>6. 评估分类器性能的方法</h2><ul>
<li>保持方法（holdout），将数据集按比例划分为不相交的训练集和验证集</li>
<li>随机二次抽样（random subsampling），多次重复保持方法</li>
<li>交叉验证（cross validation）<ul>
<li>$k$折交叉验证，把数据集分为大小相同的$k$份，在每次运行时，选择其中的一份作为验证集，而其余的全作为训练集，该过程重复$k$次，使得每份数据都用于验证恰好一次。</li>
<li>多次$k$折交叉验证，将$k$折交叉验证重复多次，比如十次十折交叉验证。</li>
<li>留一法（leave-one-out），令$k$为数据集的大小，验证集中只有一个记录。优点是使用尽可能多的训练数据并且有效的覆盖了整个数据集；缺点是整个过程重复数据集大小次数，计算开销大，而且由于验证集中只有一个记录，所以性能估计的方差较大。</li>
</ul>
</li>
<li>分层抽样（stratified sampling）</li>
<li>自助法（bootstrap）</li>
</ul>
<p>ROC曲线，横坐标为假阳率$FPR = \frac{FP}{N}$，纵坐标为真阳率$TPR = \frac{TP}{P}$。AUC面积是ROC曲线下的面积，ROC曲线一般位于$y=x$之上，所以AUC取值一般在$0.5 \sim 1$之间，值越大说明模型的性能越好。</p>
<h1 id="第五章-神经网络"><a href="#第五章-神经网络" class="headerlink" title="第五章 神经网络"></a>第五章 神经网络</h1><h2 id="1-神经网络如何学习"><a href="#1-神经网络如何学习" class="headerlink" title="1. 神经网络如何学习"></a>1. 神经网络如何学习</h2><p>神经网络通过调整权值进行学习，从而能够正确的对训练数据进行分类，然后在测试阶段对未知数据进行分类。</p>
<p>特点：</p>
<ul>
<li>神经网络需要很长时间的训练。</li>
<li>神经网络对噪声数据和不完整数据有很高的容忍度。</li>
<li>神经网络的可解释性较差。</li>
</ul>
<h2 id="2-梯度下降算法"><a href="#2-梯度下降算法" class="headerlink" title="2. 梯度下降算法"></a>2. 梯度下降算法</h2><hr>
<ul>
<li>初始化每个$w_i$为某个小的随机值</li>
<li>遇到终止条件之前做以下操作<ul>
<li>初始化每个$\Delta w_i$为0</li>
<li>对于每个训练样例$(\vec{x}, t)$，做<ul>
<li>把实例$\vec{x}$输入到此单元，计算输出$o$</li>
<li>对于线性单元的每个权$w_i$做：$\Delta w_i \leftarrow \Delta w_i + \eta(t-o) x_i$</li>
</ul>
</li>
<li>对于线性单元的每个权$w_i$，做$w_i \leftarrow w_i + \Delta w_i$</li>
</ul>
</li>
</ul>
<hr>
<p>批量梯度下降（BGD）：在每一次迭代时计算完所有的样本后进行梯度的更新。</p>
<p>随机梯度下降（SGD）：在每一次迭代时每计算完一个样本后都进行梯度的更新。</p>
<h2 id="3-反向传播算法（BP算法）"><a href="#3-反向传播算法（BP算法）" class="headerlink" title="3. 反向传播算法（BP算法）"></a>3. 反向传播算法（BP算法）</h2><p>逐层求出目标函数对各神经元权值的偏导数，构成目标函数对权值向量的梯度，作为修改权值的依据，网络的学习在权值修改过程中完成。误差达到所期望值时，网络学习结束。</p>
<h1 id="第六章-贝叶斯分类方法"><a href="#第六章-贝叶斯分类方法" class="headerlink" title="第六章 贝叶斯分类方法"></a>第六章 贝叶斯分类方法</h1><h2 id="1-根据贝叶斯理论，如何计算一个假设h成立的后验概率？"><a href="#1-根据贝叶斯理论，如何计算一个假设h成立的后验概率？" class="headerlink" title="1. 根据贝叶斯理论，如何计算一个假设h成立的后验概率？"></a>1. 根据贝叶斯理论，如何计算一个假设h成立的后验概率？</h2><p>$P(h|D) = \frac{P(D|h)P(h)}{P(D)}$</p>
<ul>
<li>$P(h)$和$P(D)$是先验概率， $P(h)$表示假设 $h$是一个正确假设的概率，$P(D)$表示在没有确定某一假设成立时$D$的概率。</li>
<li>$P(D|h)表示假设$h$成立的情况下，观察到数据$D$的概率。</li>
<li>$P(h|D)$是要求的后验概率，即给定数据集$D$上，$h$成立的概率。</li>
</ul>
<h2 id="2-极大后验假设和极大似然假设"><a href="#2-极大后验假设和极大似然假设" class="headerlink" title="2. 极大后验假设和极大似然假设"></a>2. 极大后验假设和极大似然假设</h2><ul>
<li><p>极大后验假设（MAP）</p>
<ul>
<li>在假设集$H$中寻找给定数据集$D$时，最可能的假设$h$，这样具有最大可能性的假设称为极大后验假设，$h_{MAP} = \underset{h \in H}{\arg \max } P(D \mid h) P(h)$。</li>
</ul>
</li>
<li><p>极大似然假设（ML）</p>
<ul>
<li>假定$H$中每个假设具有相同的先验概率，使$P(D \mid h)$最大的假设称为极大似然假设，$h_{ML} = \underset{h \in H}{\arg \max } P(D \mid h)$。</li>
</ul>
</li>
</ul>
<h2 id="3-最小描述长度的基本思想"><a href="#3-最小描述长度的基本思想" class="headerlink" title="3. 最小描述长度的基本思想"></a>3. 最小描述长度的基本思想</h2><p>为随机传送的消息设计一个编码，其中遇到消息$i$的概率为$p_i$，为了传输随机消息所需的传送位数最小，需要为可能性较大的消息赋予较短的编码。用最小描述长度解释极大后验假设就是使假设描述长度和给定假设下数据描述长度之和最小化的假设。$h_{max} = \underset{h \in H}{\arg \min } L_{C_H}(h) L_{C_{D \mid h}}(h)$，其中$C_H$和$C_{D \mid h}$是$H$的最优编码和给定$h$时$D$的最优编码。</p>
<h2 id="4-贝叶斯最优分类器"><a href="#4-贝叶斯最优分类器" class="headerlink" title="4. 贝叶斯最优分类器"></a>4. 贝叶斯最优分类器</h2><p>新实例的最可能分类可通过合并所有假设的预测得到，用后验概率加权。如果新实例的可能分类可取集合$V$中的任一值$v_j$，那么概率$P(v_j \mid D)$表示新实例的正确分类为$v_j$的概率：$P(v_j \mid D) = \Sigma_{h_i \in H} P(v_j \mid h_i)P(h_i \mid D)$。新实例的最优分类为使$P(v_j \mid D)$最大的$v_j$值，即$\underset{v_j \in V}{\arg \max } \Sigma_{h_i \in H} P(v_j \mid h_i)P(h_i \mid D)$。</p>
<p>贝叶斯最优分类器开销很大，需要计算每个假设的后验概率，一个可替代的、非最优的方法是Gibbs算法：按照当前的后验概率分布使用一随机抽取的假设。Gibbs算法的误分类率的期望值最多为贝叶斯最优分类器的两倍。</p>
<h2 id="5-朴素贝叶斯分类器"><a href="#5-朴素贝叶斯分类器" class="headerlink" title="5. 朴素贝叶斯分类器"></a>5. 朴素贝叶斯分类器</h2><p>贝叶斯方法的新实例的分类目标是在给定描述实例的属性值$<a_1,a_2,...,a_n>$下，得到最可能的目标值$v_{MAP} = \underset{v_j \in V}{\arg \max } P(v_j \mid a_1,a_2,…,a_n)$，利用贝叶斯公式可以重写为$v_{MAP} = \underset{v_j \in V}{\arg \max } \frac{P(a_1,a_2,…,a_n \mid v_j)P(v_j)}{P(a_1,a_2,…,a_n)}=\underset{v_j \in V}{\arg \max } P(a_1,a_2,…,a_n \mid v_j)P(v_j)$。</p>
<p>朴素贝叶斯方法就是假设给定目标值时属性值之间相互条件独立，即$P(a_1,a_2,…,a_n \mid v_j) = \prod_{i=1}^n P(a_i \mid v_j)$。朴素贝叶斯使用的方法即为$v_{NB} = \underset{v_j \in V}{\arg \max } \prod_{i=1}^n P(a_i \mid v_j) P(v_j)$。</p>
<h2 id="6-贝叶斯信念网络的预测与诊断"><a href="#6-贝叶斯信念网络的预测与诊断" class="headerlink" title="6. 贝叶斯信念网络的预测与诊断"></a>6. 贝叶斯信念网络的预测与诊断</h2><h2 id="7-偏差-方差分析"><a href="#7-偏差-方差分析" class="headerlink" title="7. 偏差-方差分析"></a>7. 偏差-方差分析</h2><h1 id="第七章-基于实例的学习"><a href="#第七章-基于实例的学习" class="headerlink" title="第七章 基于实例的学习"></a>第七章 基于实例的学习</h1><h2 id="1-k近邻学习算法"><a href="#1-k近邻学习算法" class="headerlink" title="1. k近邻学习算法"></a>1. k近邻学习算法</h2><p>假定所有的实例对应于n维空间$R^n$中的点，一个实例的最近邻是根据标准欧式距离定义的。在最近邻学习中，目标函数值可以是离散值也可以是实值。</p>
<hr>
<p>训练算法：</p>
<ul>
<li>对于每个训练样例$<x,f(x)>$，把这个样例加入列表$training _ examples$</li>
</ul>
<p>分类算法：</p>
<ul>
<li>给定一个要分类的查询实例$x_q$<ul>
<li>在$training _ examples$中选出最靠近$x_q$的$k$个实例，并用$x_1,x_2,…,x_k$表示</li>
<li>返回$\hat{f}(x_q) \leftarrow \underset{v \in V}{\arg \max } \Sigma_{i=1}^{k} \delta(v,f(x_i))$</li>
</ul>
</li>
</ul>
<p>其中，如果a=b，那么$\delta(a,b) = 1$，否则$\delta(a,b) = 0$。在实值目标函数中将公式变为$\hat{f}(x_q) \leftarrow \frac{\Sigma_{i=1}^{k} f(x_i)}{k}$。</p>
<hr>
<p>距离加权最近邻算法：将较大的权值赋给较近的近邻。</p>
<ul>
<li>$\hat{f}(x_q) \leftarrow \underset{v \in V}{\arg \max } \Sigma_{i=1}^{k} w_i \delta(v,f(x_i))$</li>
<li>$\hat{f}(x_q) \leftarrow \frac{\Sigma_{i=1}^{k} w_i f(x_i)}{\Sigma_{i=1}^{k} w_i}$</li>
</ul>
<h2 id="2-k近邻学习时为什么距离要归一化"><a href="#2-k近邻学习时为什么距离要归一化" class="headerlink" title="2. k近邻学习时为什么距离要归一化"></a>2. k近邻学习时为什么距离要归一化</h2><p>如果各个维度的量纲差距很大，那么在计算距离时模长大的维度会支配模长小的维度，造成距离失去意义。</p>
<h2 id="3-局部加权线性回归"><a href="#3-局部加权线性回归" class="headerlink" title="3. 局部加权线性回归"></a>3. 局部加权线性回归</h2><p>给定一个新的查询实例$x_q$，局部加权回归的一般做法是建立一个逼近$\hat{f}$，使$\hat{f}$拟合环绕$x_q$的邻域内的训练样例。然后用这个逼近来计算$\hat{f} (x_q)$的值，也就是为查询实例估计的目标值输出。</p>
<ul>
<li>误差函数为$E(x_{q}) = \frac{1}{2} \sum (f(x)-\hat{f}(x))^{2} K(d(x_{q}, x))$，其中$x$是$x_q$的$k$个近邻，$K(d(x_{q}, x))$是权值，是关于相距$x_q$距离的某个递减函数$K$。</li>
<li>训练法则为$\Delta w_{i}=\eta \sum K(d(x_{q}, x))(f(x)-\hat{f}(x)) a_{j}(x)$，其中$x$是$x_q$的$k$个近邻，$a_j(x)$是$x$的第$j$个属性。</li>
</ul>
<h2 id="4-基于案例的推理（CBR）与k-NN的异同"><a href="#4-基于案例的推理（CBR）与k-NN的异同" class="headerlink" title="4. 基于案例的推理（CBR）与k-NN的异同"></a>4. 基于案例的推理（CBR）与k-NN的异同</h2><p>同：</p>
<ul>
<li>都是懒惰学习的方法，把在训练数据之外的泛化推迟到遇到一个新的查询实例进行。</li>
<li>通过分析相似的实例来分类新的查询实例，忽略与查询极其不同的实例。</li>
</ul>
<p>异：</p>
<ul>
<li>CBR不把实例表示为n维空间中的实数点，而是采用更丰富的符号描述。</li>
<li>CBR检索相似实例的方法更加复杂。</li>
<li>CBR合并多个检索到的案例的过程与k-NN有很大的不同，它依赖于知识推理而不是统计方法。</li>
</ul>
<h2 id="5-懒惰学习与积极学习的区别"><a href="#5-懒惰学习与积极学习的区别" class="headerlink" title="5. 懒惰学习与积极学习的区别"></a>5. 懒惰学习与积极学习的区别</h2><p>懒惰（消极）学习：延迟了如何从训练数据中泛化的决策，直到遇到一个新的查询实例时才进行泛化。懒惰学习<strong>可以</strong>通过很多局部逼近的组合表示目标函数（局部逼近或全局逼近）。懒惰学习在训练时需要较少的计算，但在预测新查询的目标值时需要较多的计算时间。</p>
<p>积极学习：在见到新的查询之前就做好了泛化的工作。积极学习<strong>必须</strong>在训练时提交单个的全局逼近（必须全局逼近）。积极学习在训练时需要较多的时间，在预测新查询的目标值时需要较少的时间。</p>
<h1 id="第八章-集成学习"><a href="#第八章-集成学习" class="headerlink" title="第八章 集成学习"></a>第八章 集成学习</h1><h2 id="1-集成学习的定义"><a href="#1-集成学习的定义" class="headerlink" title="1. 集成学习的定义"></a>1. 集成学习的定义</h2><p>集成学习是指将许多弱学习器组合起来以获得一个强学习器的技术。</p>
<h2 id="2-集成学习的两个主要问题"><a href="#2-集成学习的两个主要问题" class="headerlink" title="2. 集成学习的两个主要问题"></a>2. 集成学习的两个主要问题</h2><ul>
<li>如何产生基学习器（基学习器要尽量准确并且多样）</li>
<li>如何合并基学习器<ul>
<li>加权投票（Weighted voteing）</li>
<li>加权平均（Weighted averaging）</li>
<li>学习组合器（learning combiner）<ul>
<li>Stacking(Wolpert)</li>
<li>RegionBoost(Maclin)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>同质（homogeneous）集成：所有的个体学习器都是同一个种类的。</p>
<p>异质（heterogeneous）集成：所有的个体学习器不全是一个种类的。</p>
<h2 id="3-Stacking基本思想和伪代码"><a href="#3-Stacking基本思想和伪代码" class="headerlink" title="3. Stacking基本思想和伪代码"></a>3. Stacking基本思想和伪代码</h2><p>Stacking算法分为两层，第一层是用不同的算法（因此Stacking一般是异质集成）形成多个弱分类器，然后将其输出用于训练第二层的元分类器，使用元分类器对第一层分类器进行组合。</p>
<p>伪代码</p>
<hr>
<p><strong>Input:</strong></p>
<p>DataSet $D = {(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$</p>
<p>First-level learning algorithm $L_1, L_2,…,L_T$</p>
<p>second-level learning algorithm $L$</p>
<p><strong>Process:</strong></p>
<p>for $t = 1,…,T$:</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;$h_t = L_t(D)$ &nbsp;&nbsp;%在数据集D上使用不同的学习算法训练第一层的分类器</p>
<p>end;</p>
<p>$D \prime = \emptyset$&nbsp;&nbsp;%创建一个新的数据集用来训练元分类器</p>
<p>for $i=1,…,m$:</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;for $t = 1,…,T$:</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$z_{it}=h_t(x_i)$&nbsp;&nbsp;%用$h_t$对实例$x_i$进行分类</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;end;</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;$D \prime = D \prime \bigcup \lbrace ((z_{i1},z_{i2},…,z_{it}),y_i) \rbrace$</p>
<p>end;</p>
<p>$h \prime = L(D \prime)$ &nbsp;&nbsp;%在数据集$D \prime$上使用算法$L$训练元分类器$h \prime$</p>
<p><strong>Output:</strong></p>
<p>$H(x) = h \prime (h_1(x), h_2(x), …, h_T(x))$</p>
<hr>
<h2 id="4-Bagging基本思想和伪代码"><a href="#4-Bagging基本思想和伪代码" class="headerlink" title="4. Bagging基本思想和伪代码"></a>4. Bagging基本思想和伪代码</h2><p>对训练样本随机抽样，让基学习器在不同的训练集进行训练而得到不同的弱分类器，最后通过投票的方式或平均的方式进行集成。（同质集成）</p>
<p>伪代码</p>
<hr>
<p>Getting $L$ samples by bootstrap sampling</p>
<p>From whice we derive:</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;$L$ classifiers $\in \lbrace -1, 1 \rbrace:c^1,c^2,…,c^L$ or </p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;$L$ Estimated probabilites $\in \lbrack -1, 1 \rbrack:p^1,p^2,…,p^L$</p>
<p>The aggregate classifier becomes</p>
<p>$c_{bag} = sign(\frac{1}{L} \Sigma_{b=1}^L c^b(x))$ or $c_{bag} = \frac{1}{L} \Sigma_{b=1}^L p^b(x)$</p>
<hr>
<h2 id="5-Boosting基本思想和伪代码"><a href="#5-Boosting基本思想和伪代码" class="headerlink" title="5. Boosting基本思想和伪代码"></a>5. Boosting基本思想和伪代码</h2><p>首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2，如此重复进行，直到弱学习器数达到事先指定的数目T。对于训练好的弱分类器，如果是分类任务按照权重进行投票，而对于回归任务进行加权，得到最终的强学习器。（同质集成）</p>
<p>伪代码</p>
<hr>
<p><strong>Input:</strong></p>
<p>Instance distribution $D$</p>
<p>Base learning algorithm $L$</p>
<p>Number of learning round $T$</p>
<p><strong>Process:</strong></p>
<p>$D_1 = D$ &nbsp;&nbsp;%初始化分布</p>
<p>for $t=1,2,…,T$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;$h_t=L(D_t)$  &nbsp;&nbsp;%在分布$D_t$上训练弱分类器$h_t$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;$\epsilon_{t}=\operatorname{Pr}_{x \sim D_{t}, y} I\left[h_{t}(x) \neq y\right]$ &nbsp;&nbsp;%计算$h_t$的错误</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;$D_{t+1}=Adjust _ Distribution\left(D_{t}, \epsilon_{t}\right)$ &nbsp;&nbsp;%调整分布，对分类错误的数据加大权重</p>
<p>end;</p>
<p><strong>Output:</strong></p>
<p>$H(x)= Combine _ Outputs \left(\left\{h_{t}(x)\right\}\right)$</p>
<hr>
<h2 id="6-为什么集成学习有效"><a href="#6-为什么集成学习有效" class="headerlink" title="6. 为什么集成学习有效"></a>6. 为什么集成学习有效</h2><ul>
<li><p>统计上：当假设空间对于可用数据量来说太大时，数据上有许多相同精度的假设，学习算法只能够选择其中一个，这样有可能导致所选假设在未见数据上的准确性很差，把多个可能假设集合起来可以降低这种风险。</p>
</li>
<li><p>计算上：许多学习算法是通过执行某种形式的局部搜索来工作的，这些搜索可能会陷入局部最优。通过从多个不同的起始点运行局部搜索构造的集成比任何单个分类器都能更好的逼近真实的目标函数。</p>
</li>
<li><p>表示上：在大多数机器学习的应用场合中实际目标假设并不在假设空间之中，如果假设空间在某种集成运算下不封闭，那么我们通过把假设空间中的一系列假设集成起来就有可能表示出不在假设空间中的目标假设。</p>
</li>
</ul>
<h1 id="第九章-分类技术"><a href="#第九章-分类技术" class="headerlink" title="第九章 分类技术"></a>第九章 分类技术</h1><h2 id="1-基于规则的分类器"><a href="#1-基于规则的分类器" class="headerlink" title="1. 基于规则的分类器"></a>1. 基于规则的分类器</h2><p>基于规则的分类器是使用一组“if…then…”规则来对记录进行分类的技术。规则的左边称为规则前件或前提，规则右边称为规则后件。一般用覆盖率（coverage）和准确率（accuracy）度量规则的质量。</p>
<h3 id="1-1-规则质量评估"><a href="#1-1-规则质量评估" class="headerlink" title="1.1 规则质量评估"></a>1.1 规则质量评估</h3><ul>
<li>覆盖率：$coverage = \frac{|A|}{|D|}$，即满足规则前件的记录所占的比例。</li>
<li>准确率：$accuracy = \frac{|A \bigcap y|}{|A|}$，即同时满足规则前件和后件的记录在满足规则前件的记录中所占的比例。</li>
</ul>
<h3 id="1-2-优点"><a href="#1-2-优点" class="headerlink" title="1.2 优点"></a>1.2 优点</h3><p>像决策树一样具有高度的表达能力；易于解释，易于生成；可以快速分类新实例，性能可与决策树相媲美。</p>
<h3 id="1-3-需要解决的问题"><a href="#1-3-需要解决的问题" class="headerlink" title="1.3 需要解决的问题"></a>1.3 需要解决的问题</h3><ul>
<li><p>一个记录可能触发多条规则（不满足互斥规则）</p>
<ul>
<li>对于有序规则集：基于规则的排序方案、基于类的排序方案</li>
<li>对于无序规则集：采用投票的方式</li>
</ul>
</li>
<li><p>一条记录可能不会触发任何规则（不满足穷举规则）</p>
<ul>
<li>使用缺省类（通常被指定为没有被现存规则覆盖的训练记录的多数类）</li>
</ul>
</li>
</ul>
<h3 id="1-4-规则建立的方法"><a href="#1-4-规则建立的方法" class="headerlink" title="1.4 规则建立的方法"></a>1.4 规则建立的方法</h3><p>规则的建立可以使用直接方法和间接方法。直接方法直接从数据中提取分类规则，如RIPPER，CN2；间接方法从其他分类模型（如决策树和神经网络）中提取分类规则，如C4.5rules。</p>
<h2 id="2-顺序覆盖算法"><a href="#2-顺序覆盖算法" class="headerlink" title="2. 顺序覆盖算法"></a>2. 顺序覆盖算法</h2><p>直接从数据中提取规则，规则基于某种评估度量以贪心的方式增长。该算法从包含多个类的数据集中一次提取一个类的规则。</p>
<p>算法开始时决策表（规则集）为空，接下来用Learn-One-Rule函数提取类C的覆盖当前训练记录集的最佳规则。如果一个规则覆盖大多数的类C训练记录，没有或仅覆盖极少的其他类训练记录（这样的规则具有高准确率，不必是高覆盖率的，因为每个类可以有多个规则），那么该规则是可取的。一旦找到这样的规则，就删掉它所覆盖的训练记录，并把新规则追加到决策表中。重复这个过程，直至满足终止条件。</p>
<h2 id="3-支持向量机"><a href="#3-支持向量机" class="headerlink" title="3. 支持向量机"></a>3. 支持向量机</h2><h1 id="第十章-聚类分析"><a href="#第十章-聚类分析" class="headerlink" title="第十章 聚类分析"></a>第十章 聚类分析</h1><h2 id="1-聚类的定义"><a href="#1-聚类的定义" class="headerlink" title="1. 聚类的定义"></a>1. 聚类的定义</h2><p>聚类分析，简称聚类，是一个把数据对象划分成子集的过程。每个子集是一个簇，使得簇中的对象彼此相似，但与其他簇中的对象不相似。（非监督的）</p>
<h2 id="2-聚类（clustering）的类型"><a href="#2-聚类（clustering）的类型" class="headerlink" title="2. 聚类（clustering）的类型"></a>2. 聚类（clustering）的类型</h2><ul>
<li><p>层次的与划分的</p>
<ul>
<li>层次聚类：允许簇有子簇</li>
<li>划分聚类：简单的将数据对象划分为不重叠的子集（簇）</li>
</ul>
</li>
<li><p>互斥的、重叠的与模糊的</p>
<ul>
<li>互斥的：每个对象都指派到单个簇</li>
<li>重叠的：一个对象同时属于不同的簇</li>
<li>模糊聚类：每个对象以一个0（绝对不属于）和1（绝对属于）之间的隶属权值属于每个簇</li>
</ul>
</li>
<li><p>完全的与部分的</p>
<ul>
<li>完全聚类：将每个对象都指派到一个簇</li>
<li>部分聚类：一些噪声、离群点等不被指派到任何一个簇</li>
</ul>
</li>
</ul>
<h2 id="3-簇（cluster）的类型"><a href="#3-簇（cluster）的类型" class="headerlink" title="3. 簇（cluster）的类型"></a>3. 簇（cluster）的类型</h2><ul>
<li>明显分离的。每个点到同簇中任意点的距离比到不同簇中所有点的距离更近。</li>
<li>基于中心的。每个点到其簇中心的距离比到任何其他簇中心的距离更近。</li>
<li>基于邻近的。每个点到该簇中至少一个点的距离比到不同簇中任意点的距离更近。</li>
<li>基于密度的。簇是被低密度区域分开的高密度区域。</li>
<li>概念簇。簇中的点具有由整个点集导出的某种一般共同性质。</li>
</ul>
<h2 id="4-层次聚类"><a href="#4-层次聚类" class="headerlink" title="4. 层次聚类"></a>4. 层次聚类</h2><h3 id="4-1-层次聚类的两种主要类型"><a href="#4-1-层次聚类的两种主要类型" class="headerlink" title="4.1 层次聚类的两种主要类型"></a>4.1 层次聚类的两种主要类型</h3><ul>
<li>凝聚的：从点作为个体簇开始，每一步合并两个最接近的簇。需要定义簇的邻近性的概念。</li>
<li>分裂的：从包含所有点的某个簇开始，每一步分裂一个簇，直到只剩下单点簇。需要确定每一步分裂哪个簇，以及如何分裂。</li>
</ul>
<h3 id="4-2-定义簇之间的邻近性"><a href="#4-2-定义簇之间的邻近性" class="headerlink" title="4.2 定义簇之间的邻近性"></a>4.2 定义簇之间的邻近性</h3><ul>
<li>单链（MIN）：两个簇的邻近度定义为两个不同簇中任意两点之间的最短距离（最大相似度）。</li>
</ul>
<ul>
<li>全链（MAX）：两个簇的邻近度定义为两个不同簇中任意两点之间的最长距离（最小相似度）。</li>
</ul>
<ul>
<li><p>组平均：两个簇的邻近度定义为不同簇的所有点对邻近度的平均值。</p>
<ul>
<li>$proximity(C_i,C_j)=\frac{\Sigma_{x \in C_i y \in C_j} proximity(x,y)}{m_i \times m_j}$</li>
</ul>
</li>
<li><p>Ward方法：两个簇合并时导致的平方误差的增量。</p>
<ul>
<li>$\Delta(A, B)=\sum_{i \in A \cup B}\left|\vec{x}_{i}-\vec{m}_{A \cup B}\right|^{2}-\sum_{i \in A}\left|\vec{x}_{i}-\vec{m}_{A}\right|^{2}-\sum_{i \in B}\left|\vec{x}_{i}-\vec{m}_{B}\right|^{2}$ $=\frac{n_{A} n_{B}}{n_{A}+n_{B}}\left|\vec{m}_{A}-\vec{m}_{B}\right|^{2}$</li>
</ul>
</li>
</ul>
<p>单链擅长处理非椭圆形的簇，但对噪声和离群点很敏感。全链对噪声和离群点不太敏感，但是它可能使大的簇破裂，并偏好球形。</p>
<h3 id="4-3-层次聚类的缺点"><a href="#4-3-层次聚类的缺点" class="headerlink" title="4.3 层次聚类的缺点"></a>4.3 层次聚类的缺点</h3><ul>
<li>一旦决定合并两个簇，就不能撤销</li>
<li>没有直接最小化目标函数</li>
<li>不同的方案存在以下一个或多个问题<ul>
<li>对噪声和异常值的敏感性</li>
<li>难以处理不同大小的簇和凸形状</li>
<li>破坏大型的簇</li>
</ul>
</li>
</ul>
<h2 id="5-k均值和k中心点算法"><a href="#5-k均值和k中心点算法" class="headerlink" title="5. k均值和k中心点算法"></a>5. k均值和k中心点算法</h2><h3 id="5-1-k-means算法"><a href="#5-1-k-means算法" class="headerlink" title="5.1 k-means算法"></a>5.1 k-means算法</h3><p>首先，选择k个初始质心，其中k是用户指定的参数，即所期望的簇的个数。每个点指派到最近的质心，而指派到一个质心的点集为一个簇。然后，根据指派到簇的点，更新每个簇的质心。重复指派和更新步骤，直到质心不再发生变化。</p>
<hr>
<p>选择k个点作为初始质心</p>
<p><strong>repeat</strong></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;将每个点指派到最近的质心，形成k个簇</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;重新计算每个簇的质心</p>
<p><strong>until</strong></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;质心不再发生变化</p>
<hr>
<p>考虑邻近度度量为欧几里得距离的数据，使用误差的平方和（SSE）作为度量聚类质量的目标函数。假设簇划分为$C_1,C_2,…,C_k$，则目标函数为$SSE=\sum_{i=1}^{k} \sum_{x \in C_{i}} d i s t^{2}\left(c_{i}, x\right)$，其中$c_i$表示第$i$个簇的质心（均值），计算公式为$c_i=\frac{1}{m_i} \sum_{x \in C_i} x$，例如3个二维点$(1,1),(2,3),(6,2)$的质心是$((1+2+6)/3, (1+3+2)/3)=(3,2)$。公式中$x$是一个点，$C_i$是第$i$个簇，$c_i$是簇$C_i$的质心，$m_i$是第$i$个簇中点的个数。</p>
<p>k-means算法的结果依赖于初始簇中心的随机选择，实践中为了得到好的结果，通常以不同的初始簇中心多次运行k-means算法，然后选取具有最小SSE的簇集。</p>
<p>用后处理降低SSE：</p>
<p>总SSE只不过是每个簇SSE的和，通过在簇上进行诸如分裂和合并的操作，可以改变总SSE。</p>
<ul>
<li><p>通过增加簇的个数来降低总SSE的策略</p>
<ul>
<li>分裂一个簇，通常选择具有最大SSE的簇</li>
<li>引进一个新的质心，通常选择离所有簇质心最远的点</li>
</ul>
</li>
<li><p>通过减少簇的个数来降低总SSE的策略</p>
<ul>
<li>拆散一个簇，删除簇的对应质心</li>
<li>合并两个簇，通常选择质心最接近的两个簇</li>
</ul>
</li>
</ul>
<p>k-means算法的缺点：</p>
<ul>
<li>通常停止在局部最优</li>
<li>并不适合所有的数据类型（仅限于具有中心概念的数据）</li>
<li>不能处理非球形簇、不同尺寸和不同密度的簇</li>
<li>不能处理噪声和离群点</li>
</ul>
<h3 id="5-2-k-medoids算法"><a href="#5-2-k-medoids算法" class="headerlink" title="5.2 k-medoids算法"></a>5.2 k-medoids算法</h3><p>k-means算法的改进，降低它对离群点的敏感性。不采用簇中对象的均值作为参照点，而是挑选实际对象来代表簇，每个簇使用一个代表对象，其余的每个对象被分配到与其最为相似的代表性对象所在的簇中。</p>
<p>该算法的一个实现是围绕中心点（PAM）算法。随机选择代表对象，然后考虑用一个非代表对象替换一个代表对象是否能够提高聚类质量。尝试所有可能的替换，直到结果聚类的质量不可能被任何替换提高。</p>
<h2 id="6-DBSCAN算法"><a href="#6-DBSCAN算法" class="headerlink" title="6. DBSCAN算法"></a>6. DBSCAN算法</h2><p>DBSCAN is a <strong>D</strong>ensity <strong>B</strong>ased <strong>S</strong>patial <strong>C</strong>lustering of <strong>A</strong>pplications with <strong>N</strong>oise.具有噪声应用的基于密度的空间聚类。</p>
<p>根据基于中心的密度进行点分类：</p>
<ul>
<li>核心点（core point）：这些点在基于密度的簇内部。点的邻域由距离函数和用户指定的距离参数 $Eps$ 决定。核心点的定义是，如果该点的给定邻域内的点的个数超过给定的阈值 $MinPts$ , 其中 $MinPts$ 也是一个用户指定的参数。</li>
<li>边界点（border point）：边界点不是核心点，但它落在某个核心点的邻域内。边界点可能落在多个核心点的邻域内。</li>
<li>噪声点（noise point）：噪声点是既非核心点也非边界点的任何点。</li>
</ul>
<hr>
<p><strong>repeat:</strong></p>
<p>从数据库中抽出一个未处理的点；</p>
<p>IF 抽出的点是核心点 THEN 找出所有从该点 _密度可达_ 的对象，形成一个簇；</p>
<p>ELSE 抽出的点是边缘点(非核心对象)，跳出本次循环，寻找下一个点；</p>
<p>UNTIL 所有的点都被处理。</p>
<hr>
<p>优点：</p>
<ul>
<li>可以对抗噪声</li>
<li>能够处理任意形状和大小的簇</li>
</ul>
<p>缺点：</p>
<ul>
<li>密度变化太大时不能处理</li>
<li>对于高维数据也不能很好的工作</li>
</ul>
<h2 id="7-聚类评估"><a href="#7-聚类评估" class="headerlink" title="7. 聚类评估"></a>7. 聚类评估</h2><p>用于评估簇的各方面的评估度量或指标一般分成如下三类：</p>
<ul>
<li>非监督的。聚类结构的优良性度量，不考虑外部信息。例如，SSE。簇的有效性的非监督度量常常可以进一步分成两类: 簇的凝聚性（cluster cohesion）度量，确定簇中对象如何密切相关；簇的分离性（cluster separation）度量确定某个簇不同于其他簇的地方。非监督度量通常称为内部指标（internal index)，因为它们仅使用出现在数据集中的信息。</li>
</ul>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="1-协方差矩阵的计算"><a href="#1-协方差矩阵的计算" class="headerlink" title="1. 协方差矩阵的计算"></a>1. 协方差矩阵的计算</h2><p>对于一组数据，比如$x_1(1,2), x_2(2,6), x_3(4,2), x_4(5,2)$，因为数据是二维的（即两列），所以协方差矩阵是一个$2 \times 2$的矩阵。协方差矩阵的元素$(i,j)=$（第$i$维的所有元素-第$i$维的均值） $\cdot$ （第$j$维的所有元素-第$j$维的均值）$/$ 行数$-1$（即样本数$-1$）。</p>
<ul>
<li>协方差矩阵是一个对称矩阵</li>
<li>对角线元素$(i,i)$为第$i$维数据的方差</li>
<li>非对角线元素$(i,j)$为第$i$维和第$j$维的协方差</li>
</ul>
<h2 id="2-标准差"><a href="#2-标准差" class="headerlink" title="2. 标准差"></a>2. 标准差</h2><p>总体标准差：$\sigma = \sqrt{\frac{\Sigma_{i=1}^{n}(x_i-\overline{x})^2}{n}}$</p>
<p>样本标准差：$\sigma = \sqrt{\frac{\Sigma_{i=1}^{n}(x_i-\overline{x})^2}{n-1}}$</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/inabaraku/article/details/51862151?utm_source=blogxgwz4">由霍夫丁不等式论证机器学习的可行性</a></p>

      
    </div>

    
      


    

    
    
    

    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-heartbeat"></i>感谢您的阅读-------------</div>
    
</div>
      
    </div>

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>觉得不错? 打赏一下!</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.png" alt="lihui 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="lihui 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>lihui</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://wheliosc.github.io/mldm.html" title="机器学习与数据挖掘">https://wheliosc.github.io/mldm.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" rel="tag"><i class="fa fa-tag"></i> 数据挖掘</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/LAMP" rel="next" title="LAMP">
                <i class="fa fa-chevron-left"></i> LAMP
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/springboot-deploy" rel="prev" title="SpringBoot项目部署到服务器">
                SpringBoot项目部署到服务器 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>
  




        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/uploads/avatar.png"
                alt="lihui" />
            
              <p class="site-author-name" itemprop="name">lihui</p>
              <p class="site-description motion-element" itemprop="description">星光即使微弱也会为我照亮前途</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/%20%7C%7C%20archive">
                
                    <span class="site-state-item-count">58</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">32</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/WHeLiosC" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:2469660964@qq.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-globe"></i>
                不点一下嘛
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.zhihu.com/people/li-san-sui-ta-hen-pi/activities" title="知乎" target="_blank">知乎</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://space.bilibili.com/39273843/#/" title="B站" target="_blank">B站</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://wheliosc.github.io/about/" title="关于" target="_blank">关于</a>
                  </li>
                
              </ul>
          
<div id="days"></div>
<script>
function show_date_time(){
window.setTimeout("show_date_time()", 1000);
BirthDay=new Date("06/04/2018 15:13:14");
today=new Date();
timeold=(today.getTime()-BirthDay.getTime());
sectimeold=timeold/1000
secondsold=Math.floor(sectimeold);
msPerDay=24*60*60*1000
e_daysold=timeold/msPerDay
daysold=Math.floor(e_daysold);
e_hrsold=(e_daysold-daysold)*24;
hrsold=setzero(Math.floor(e_hrsold));
e_minsold=(e_hrsold-hrsold)*60;
minsold=setzero(Math.floor((e_hrsold-hrsold)*60));
seconds=setzero(Math.floor((e_minsold-minsold)*60));
document.getElementById('days').innerHTML="已运行"+daysold+"天"+hrsold+"小时"+minsold+"分"+seconds+"秒";
}
function setzero(i){
if (i<10)
{i="0" + i};
return i;
}
show_date_time();
</script>
            </div>
          

          
          
          
          
            
          
          
<div id="days"></div>
<script>
function show_date_time(){
window.setTimeout("show_date_time()", 1000);
BirthDay=new Date("06/04/2018 15:13:14");
today=new Date();
timeold=(today.getTime()-BirthDay.getTime());
sectimeold=timeold/1000
secondsold=Math.floor(sectimeold);
msPerDay=24*60*60*1000
e_daysold=timeold/msPerDay
daysold=Math.floor(e_daysold);
e_hrsold=(e_daysold-daysold)*24;
hrsold=setzero(Math.floor(e_hrsold));
e_minsold=(e_hrsold-hrsold)*60;
minsold=setzero(Math.floor((e_hrsold-hrsold)*60));
seconds=setzero(Math.floor((e_minsold-minsold)*60));
document.getElementById('days').innerHTML="已运行"+daysold+"天"+hrsold+"小时"+minsold+"分"+seconds+"秒";
}
function setzero(i){
if (i<10)
{i="0" + i};
return i;
}
show_date_time();
</script>
        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0-%E5%BC%95%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">第一章 引言</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#0-%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98"><span class="nav-number">1.1.</span> <span class="nav-text">0. 什么是数据挖掘</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E6%95%B0%E6%8D%AE%E4%B8%AD%E7%9A%84%E7%9F%A5%E8%AF%86%E5%8F%91%E7%8E%B0%E5%8C%85%E6%8B%AC%E5%93%AA%E5%87%A0%E4%B8%AA%E6%AD%A5%E9%AA%A4"><span class="nav-number">1.2.</span> <span class="nav-text">1. 数据中的知识发现包括哪几个步骤</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%9A%84%E5%BA%94%E7%94%A8"><span class="nav-number">1.3.</span> <span class="nav-text">2. 数据挖掘的应用</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8F%AF%E8%A1%8C%E6%80%A7"><span class="nav-number">2.</span> <span class="nav-text">第二章 学习的可行性</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Hoeffding%E2%80%99s-Inequality%EF%BC%88%E9%9C%8D%E5%A4%AB%E6%B1%80%E4%B8%8D%E7%AD%89%E5%BC%8F%EF%BC%89"><span class="nav-number">2.1.</span> <span class="nav-text">1. Hoeffding’s Inequality（霍夫汀不等式）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E7%94%A8%E9%9C%8D%E5%A4%AB%E6%B1%80%E4%B8%8D%E7%AD%89%E5%BC%8F%E8%AF%B4%E6%98%8E%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8F%AF%E8%A1%8C%E6%80%A7"><span class="nav-number">2.2.</span> <span class="nav-text">2. 用霍夫汀不等式说明学习的可行性</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0-%E6%95%B0%E6%8D%AE%E5%92%8C%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">3.</span> <span class="nav-text">第三章 数据和数据预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%B1%9E%E6%80%A7%E7%B1%BB%E5%9E%8B%E5%92%8C%E5%8F%AF%E8%BF%9B%E8%A1%8C%E7%9A%84%E6%93%8D%E4%BD%9C"><span class="nav-number">3.1.</span> <span class="nav-text">1. 属性类型和可进行的操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%B1%9E%E6%80%A7"><span class="nav-number">3.2.</span> <span class="nav-text">2. 非对称属性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%92%8C%E7%9B%B8%E5%BC%82%E6%80%A7%E5%BA%A6%E9%87%8F"><span class="nav-number">3.3.</span> <span class="nav-text">3. 相似性和相异性度量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E6%95%B0%E6%8D%AE%E5%AF%B9%E8%B1%A1%E4%B9%8B%E9%97%B4%E7%9A%84%E7%9B%B8%E5%BC%82%E5%BA%A6"><span class="nav-number">3.3.1.</span> <span class="nav-text">3.1 数据对象之间的相异度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E4%BA%8C%E5%85%83%E6%95%B0%E6%8D%AE%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F"><span class="nav-number">3.3.2.</span> <span class="nav-text">3.2 二元数据的相似性度量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E7%BB%84%E5%90%88%E5%BC%82%E7%A7%8D%E5%B1%9E%E6%80%A7%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6"><span class="nav-number">3.3.3.</span> <span class="nav-text">3.3 组合异种属性的相似度</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">3.4.</span> <span class="nav-text">4. 数据预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E6%95%B0%E6%8D%AE%E6%B8%85%E7%90%86"><span class="nav-number">3.4.1.</span> <span class="nav-text">4.1 数据清理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-1-%E5%A1%AB%E5%85%85%E7%BC%BA%E5%A4%B1%E5%80%BC%EF%BC%88missing-data%EF%BC%89"><span class="nav-number">3.4.1.1.</span> <span class="nav-text">4.1.1 填充缺失值（missing data）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-2-%E5%85%89%E6%BB%91%E5%99%AA%E5%A3%B0%EF%BC%88noise%EF%BC%89"><span class="nav-number">3.4.1.2.</span> <span class="nav-text">4.1.2 光滑噪声（noise）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-3-%E8%AF%86%E5%88%AB%E7%A6%BB%E7%BE%A4%E7%82%B9%EF%BC%88outlier%EF%BC%89"><span class="nav-number">3.4.1.3.</span> <span class="nav-text">4.1.3 识别离群点（outlier）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-4-%E7%BA%A0%E6%AD%A3%E6%95%B0%E6%8D%AE%E4%B8%AD%E7%9A%84%E4%B8%8D%E4%B8%80%E8%87%B4"><span class="nav-number">3.4.1.4.</span> <span class="nav-text">4.1.4 纠正数据中的不一致</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90"><span class="nav-number">3.4.2.</span> <span class="nav-text">4.2 数据集成</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-1-%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E9%97%AE%E9%A2%98"><span class="nav-number">3.4.2.1.</span> <span class="nav-text">4.2.1 实体识别问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-2-%E5%86%97%E4%BD%99%E5%92%8C%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90"><span class="nav-number">3.4.2.2.</span> <span class="nav-text">4.2.2 冗余和相关分析</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-%E6%95%B0%E6%8D%AE%E5%BD%92%E7%BA%A6"><span class="nav-number">3.4.3.</span> <span class="nav-text">4.3 数据归约</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-1-%E6%95%B0%E6%8D%AE%E8%81%9A%E5%90%88"><span class="nav-number">3.4.3.1.</span> <span class="nav-text">4.3.1 数据聚合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-2-%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9"><span class="nav-number">3.4.3.2.</span> <span class="nav-text">4.3.2 数据压缩</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-3-%E6%95%B0%E9%87%8F%E8%A7%84%E7%BA%A6"><span class="nav-number">3.4.3.3.</span> <span class="nav-text">4.3.3 数量规约</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-4-%E7%BB%B4%E5%BD%92%E7%BA%A6"><span class="nav-number">3.4.3.4.</span> <span class="nav-text">4.3.4 维归约</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-%E6%95%B0%E6%8D%AE%E5%8F%98%E6%8D%A2%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%A6%BB%E6%95%A3%E5%8C%96"><span class="nav-number">3.4.4.</span> <span class="nav-text">4.4 数据变换与数据离散化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-1-%E8%A7%84%E8%8C%83%E5%8C%96"><span class="nav-number">3.4.4.1.</span> <span class="nav-text">4.4.1 规范化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-2-%E7%A6%BB%E6%95%A3%E5%8C%96"><span class="nav-number">3.4.4.2.</span> <span class="nav-text">4.4.2 离散化</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%86%B3%E7%AD%96%E6%A0%91%E5%AD%A6%E4%B9%A0"><span class="nav-number">4.</span> <span class="nav-text">第四章 决策树学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%86%B3%E7%AD%96%E6%A0%91%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3"><span class="nav-number">4.1.</span> <span class="nav-text">1. 决策树学习的基本思想</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E6%9C%80%E4%BD%B3%E5%88%92%E5%88%86"><span class="nav-number">4.2.</span> <span class="nav-text">2. 如何选择最佳划分</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="nav-number">4.3.</span> <span class="nav-text">3. 过拟合和欠拟合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%AF%B9%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="nav-number">4.4.</span> <span class="nav-text">4. 缺失值对决策树的影响</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5%EF%BC%88confusion-matrix%EF%BC%89"><span class="nav-number">4.5.</span> <span class="nav-text">5. 混淆矩阵（confusion matrix）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-%E8%AF%84%E4%BC%B0%E5%88%86%E7%B1%BB%E5%99%A8%E6%80%A7%E8%83%BD%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">4.6.</span> <span class="nav-text">6. 评估分类器性能的方法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">5.</span> <span class="nav-text">第五章 神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0"><span class="nav-number">5.1.</span> <span class="nav-text">1. 神经网络如何学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="nav-number">5.2.</span> <span class="nav-text">2. 梯度下降算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%EF%BC%88BP%E7%AE%97%E6%B3%95%EF%BC%89"><span class="nav-number">5.3.</span> <span class="nav-text">3. 反向传播算法（BP算法）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E5%85%AD%E7%AB%A0-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95"><span class="nav-number">6.</span> <span class="nav-text">第六章 贝叶斯分类方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E6%A0%B9%E6%8D%AE%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%90%86%E8%AE%BA%EF%BC%8C%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E4%B8%80%E4%B8%AA%E5%81%87%E8%AE%BEh%E6%88%90%E7%AB%8B%E7%9A%84%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%EF%BC%9F"><span class="nav-number">6.1.</span> <span class="nav-text">1. 根据贝叶斯理论，如何计算一个假设h成立的后验概率？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E6%9E%81%E5%A4%A7%E5%90%8E%E9%AA%8C%E5%81%87%E8%AE%BE%E5%92%8C%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E5%81%87%E8%AE%BE"><span class="nav-number">6.2.</span> <span class="nav-text">2. 极大后验假设和极大似然假设</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E6%9C%80%E5%B0%8F%E6%8F%8F%E8%BF%B0%E9%95%BF%E5%BA%A6%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3"><span class="nav-number">6.3.</span> <span class="nav-text">3. 最小描述长度的基本思想</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%9C%80%E4%BC%98%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">6.4.</span> <span class="nav-text">4. 贝叶斯最优分类器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">6.5.</span> <span class="nav-text">5. 朴素贝叶斯分类器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BF%A1%E5%BF%B5%E7%BD%91%E7%BB%9C%E7%9A%84%E9%A2%84%E6%B5%8B%E4%B8%8E%E8%AF%8A%E6%96%AD"><span class="nav-number">6.6.</span> <span class="nav-text">6. 贝叶斯信念网络的预测与诊断</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90"><span class="nav-number">6.7.</span> <span class="nav-text">7. 偏差-方差分析</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%B8%83%E7%AB%A0-%E5%9F%BA%E4%BA%8E%E5%AE%9E%E4%BE%8B%E7%9A%84%E5%AD%A6%E4%B9%A0"><span class="nav-number">7.</span> <span class="nav-text">第七章 基于实例的学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-k%E8%BF%91%E9%82%BB%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="nav-number">7.1.</span> <span class="nav-text">1. k近邻学习算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-k%E8%BF%91%E9%82%BB%E5%AD%A6%E4%B9%A0%E6%97%B6%E4%B8%BA%E4%BB%80%E4%B9%88%E8%B7%9D%E7%A6%BB%E8%A6%81%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">7.2.</span> <span class="nav-text">2. k近邻学习时为什么距离要归一化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">7.3.</span> <span class="nav-text">3. 局部加权线性回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E5%9F%BA%E4%BA%8E%E6%A1%88%E4%BE%8B%E7%9A%84%E6%8E%A8%E7%90%86%EF%BC%88CBR%EF%BC%89%E4%B8%8Ek-NN%E7%9A%84%E5%BC%82%E5%90%8C"><span class="nav-number">7.4.</span> <span class="nav-text">4. 基于案例的推理（CBR）与k-NN的异同</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E6%87%92%E6%83%B0%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%A7%AF%E6%9E%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">7.5.</span> <span class="nav-text">5. 懒惰学习与积极学习的区别</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E5%85%AB%E7%AB%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0"><span class="nav-number">8.</span> <span class="nav-text">第八章 集成学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="nav-number">8.1.</span> <span class="nav-text">1. 集成学习的定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%A4%E4%B8%AA%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98"><span class="nav-number">8.2.</span> <span class="nav-text">2. 集成学习的两个主要问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Stacking%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3%E5%92%8C%E4%BC%AA%E4%BB%A3%E7%A0%81"><span class="nav-number">8.3.</span> <span class="nav-text">3. Stacking基本思想和伪代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Bagging%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3%E5%92%8C%E4%BC%AA%E4%BB%A3%E7%A0%81"><span class="nav-number">8.4.</span> <span class="nav-text">4. Bagging基本思想和伪代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Boosting%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3%E5%92%8C%E4%BC%AA%E4%BB%A3%E7%A0%81"><span class="nav-number">8.5.</span> <span class="nav-text">5. Boosting基本思想和伪代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%9C%89%E6%95%88"><span class="nav-number">8.6.</span> <span class="nav-text">6. 为什么集成学习有效</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%B9%9D%E7%AB%A0-%E5%88%86%E7%B1%BB%E6%8A%80%E6%9C%AF"><span class="nav-number">9.</span> <span class="nav-text">第九章 分类技术</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">9.1.</span> <span class="nav-text">1. 基于规则的分类器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E8%A7%84%E5%88%99%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BC%B0"><span class="nav-number">9.1.1.</span> <span class="nav-text">1.1 规则质量评估</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E4%BC%98%E7%82%B9"><span class="nav-number">9.1.2.</span> <span class="nav-text">1.2 优点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-%E9%9C%80%E8%A6%81%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">9.1.3.</span> <span class="nav-text">1.3 需要解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-%E8%A7%84%E5%88%99%E5%BB%BA%E7%AB%8B%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">9.1.4.</span> <span class="nav-text">1.4 规则建立的方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E9%A1%BA%E5%BA%8F%E8%A6%86%E7%9B%96%E7%AE%97%E6%B3%95"><span class="nav-number">9.2.</span> <span class="nav-text">2. 顺序覆盖算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="nav-number">9.3.</span> <span class="nav-text">3. 支持向量机</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E7%AB%A0-%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90"><span class="nav-number">10.</span> <span class="nav-text">第十章 聚类分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E8%81%9A%E7%B1%BB%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="nav-number">10.1.</span> <span class="nav-text">1. 聚类的定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E8%81%9A%E7%B1%BB%EF%BC%88clustering%EF%BC%89%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="nav-number">10.2.</span> <span class="nav-text">2. 聚类（clustering）的类型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E7%B0%87%EF%BC%88cluster%EF%BC%89%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="nav-number">10.3.</span> <span class="nav-text">3. 簇（cluster）的类型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB"><span class="nav-number">10.4.</span> <span class="nav-text">4. 层次聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E7%9A%84%E4%B8%A4%E7%A7%8D%E4%B8%BB%E8%A6%81%E7%B1%BB%E5%9E%8B"><span class="nav-number">10.4.1.</span> <span class="nav-text">4.1 层次聚类的两种主要类型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E5%AE%9A%E4%B9%89%E7%B0%87%E4%B9%8B%E9%97%B4%E7%9A%84%E9%82%BB%E8%BF%91%E6%80%A7"><span class="nav-number">10.4.2.</span> <span class="nav-text">4.2 定义簇之间的邻近性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E7%9A%84%E7%BC%BA%E7%82%B9"><span class="nav-number">10.4.3.</span> <span class="nav-text">4.3 层次聚类的缺点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-k%E5%9D%87%E5%80%BC%E5%92%8Ck%E4%B8%AD%E5%BF%83%E7%82%B9%E7%AE%97%E6%B3%95"><span class="nav-number">10.5.</span> <span class="nav-text">5. k均值和k中心点算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-k-means%E7%AE%97%E6%B3%95"><span class="nav-number">10.5.1.</span> <span class="nav-text">5.1 k-means算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-k-medoids%E7%AE%97%E6%B3%95"><span class="nav-number">10.5.2.</span> <span class="nav-text">5.2 k-medoids算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-DBSCAN%E7%AE%97%E6%B3%95"><span class="nav-number">10.6.</span> <span class="nav-text">6. DBSCAN算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-%E8%81%9A%E7%B1%BB%E8%AF%84%E4%BC%B0"><span class="nav-number">10.7.</span> <span class="nav-text">7. 聚类评估</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B6%E4%BB%96"><span class="nav-number">11.</span> <span class="nav-text">其他</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="nav-number">11.1.</span> <span class="nav-text">1. 协方差矩阵的计算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E6%A0%87%E5%87%86%E5%B7%AE"><span class="nav-number">11.2.</span> <span class="nav-text">2. 标准差</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">12.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        
<div>
<script type="text/javascript" src="https://api.imjad.cn/hitokoto/?cat=a&charset=utf-8&length=50&encode=js&fun=sync&source="></script><div id="hitokoto"><script>hitokoto()</script></div>
</div>

<div class="copyright">&copy 2018 – <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lihui</span>
</div>
<div class="website-info">
  
    
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
      本站共
    </span>
    
    <span title="站点总字数">181k
      字 
    </span>
  

  <span class="post-meta-divider">|</span>
  <span class="post-meta-item-icon">
    电脑食用最佳~
  </span>
  <span><i class="fa fa-smile-wink"></i></span>
</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="总访客量">
      <i class="fa fa-user-circle"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
    <i class="fa fa-eye"></i>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.4.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.4.2"></script>





  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.4.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.2"></script>



  







  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  
  
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: true,
        notify: false,
        appId: 'gO4OH9eyNSmwyjxHl4fKt2fq-gzGzoHsz',
        appKey: 'p9ixt8c10XVUCMyjMB5Xfge7',
        placeholder: 'Just go go',
        avatar:'mm',
        meta:guest,
        pageSize:'10' || 10,
        visitor: false
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  

  


  
  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
        
          if(result)$(this).text('复制成功')
          else $(this).text('复制失败')
        
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('复制')
        }, 300)
      }).append(e)
    })
  </script>



  <!-- 修改blog背景 -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
  <script>
  $("body").backstretch("https://github-1259166513.cos.ap-beijing.myqcloud.com/5e54a7b89e2c3bb656ef47e107cd0cbe.png");
  </script>

  <!-- 页面点击小红心 -->
  <script type="text/javascript" src="/js/src/clicklove.js"></script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":70,"height":140},"mobile":{"show":false},"log":false});</script><!-- hexo-inject:begin --><!-- hexo-inject:end --></body>
</html>